import base64
import json
import os
import time

from utils.logging_config import get_logger

# from dotenv import load_dotenv
# load_dotenv()
# api_key = os.getenv("OPENAI_API_KEY")


logger = get_logger(__name__)

from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain.storage import InMemoryStore
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import OpenAIEmbeddings

from utils.LLM_Tool import gpt_4o, text_embedding_3_large
from utils.summarize import encode_image  # Updated import
from utils.vector_store import resize_base64_image  # Updated import

# çµ±ä¸€å®šç¾©è³‡æ–™åº«è·¯å¾‘ - æ›´æ–°ç‚º FAISS
DATABASE_DIR = "./database"
FAISS_INDEX_PATH = f"{DATABASE_DIR}/faiss_store"  # Changed from VECTORSTORE_PATH
FAISS_INDEX_NAME = "mm_rag_faiss_index"  # FAISS index name
DOCSTORE_MAPPING_PATH = f"{DATABASE_DIR}/docstore_mapping.json"
IMAGE_FIGURES_PATH = f"{DATABASE_DIR}/figures/"


def split_image_text_types(docs, figure_dir):
    """
    å°‡ MultiVectorRetriever æª¢ç´¢å›ä¾†çš„åŸå§‹å…§å®¹åˆ†ç‚ºåœ–ç‰‡ (base64) èˆ‡æ–‡å­—/è¡¨æ ¼ã€‚
    docs: æª¢ç´¢å›ä¾†çš„æ–‡ä»¶åˆ—è¡¨ (åŒ…å«åŸå§‹æ–‡å­—ã€è¡¨æ ¼å­—ä¸²ï¼Œæˆ–å„ç¨®é¡å‹çš„å­—å…¸)
    å›å‚³ï¼šdictï¼Œå« imagesï¼ˆbase64 åœ–ç‰‡åˆ—è¡¨ï¼‰ã€textsï¼ˆæ–‡å­—/è¡¨æ ¼åˆ—è¡¨ï¼‰
    """
    b64_images = []
    texts = []
    page_summaries = []
    slide_summaries = []  # æ–°å¢ï¼šå¹»ç‡ˆç‰‡æ‘˜è¦åˆ—è¡¨

    for doc in docs:
        if isinstance(
            doc, Document
        ):  # Handle cases where retriever might still wrap in Document
            doc_content = doc.page_content
        else:
            doc_content = doc  # Assume it's the raw content from the docstore

        # è™•ç†ä¸åŒé¡å‹çš„å­—å…¸å…§å®¹
        if isinstance(doc_content, dict):
            # æª¢æŸ¥å­—å…¸çš„é¡å‹
            doc_type = doc_content.get("type", "")

            # è™•ç†åœ–ç‰‡é¡å‹
            if doc_type == "image" or (
                "filename" in doc_content and "type" not in doc_content
            ):
                img_filename = doc_content["filename"]
                # æ–‡ä»¶åæ˜¯ç›¸å°æ–¼ç”¨æˆ¶è³‡æ–™å¤¾çš„è·¯å¾‘
                base_dir = os.path.dirname(figure_dir)
                img_path = os.path.join(base_dir, img_filename)

                # æª¢æŸ¥åœ–ç‰‡æ˜¯å¦å­˜åœ¨
                if os.path.exists(img_path):
                    try:
                        # Encode the image from file path
                        b64_img = encode_image(img_path)
                        # Resize the base64 image
                        resized_b64_img = resize_base64_image(
                            b64_img, size=(1300, 600)
                        )  # Adjust size as needed
                        b64_images.append(resized_b64_img)
                    except Exception as e:
                        logger.warning(f"âš ï¸ Error processing image file {img_path}: {e}")
                        if os.path.isfile(img_path):
                            file_size = os.path.getsize(img_path)
                            logger.warning(f"   File exists, size: {file_size} bytes")
                            if file_size == 0:
                                logger.warning("   Warning: File size is 0 bytes!")
                        else:
                            logger.warning("   Strange: Path exists but is not a file!")
                else:
                    # å¢å¼·åœ–ç‰‡æœªæ‰¾åˆ°çš„è¨ºæ–·è¨Šæ¯
                    logger.warning(f"âš ï¸ Image file not found: {img_path}")

                    # æª¢æŸ¥å­è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨
                    doc_folder = os.path.dirname(img_path)
                    if not os.path.exists(doc_folder):
                        logger.warning(
                            f"   Document folder does not exist: {doc_folder}"
                        )
                    else:
                        logger.warning(
                            "   Document folder exists, but image file is missing"
                        )
                        # åˆ—å‡ºå­è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æª”æ¡ˆä»¥é€²è¡Œè¨ºæ–·
                        folder_files = os.listdir(doc_folder)
                        logger.warning(
                            f"   Files in folder ({len(folder_files)}): {', '.join(folder_files[:5])}"
                            + (
                                f"... and {len(folder_files)-5} more"
                                if len(folder_files) > 5
                                else ""
                            )
                        )

            # è™•ç† PDF é é¢æ‘˜è¦é¡å‹
            elif doc_type == "pdf_page":
                page_id = doc_content.get("page_id", "unknown_page")
                summary = doc_content.get("summary", "ç„¡æ‘˜è¦")
                page_summaries.append(f"ã€é é¢: {page_id}ã€‘\n{summary}")

            # æ–°å¢ï¼šè™•ç† PPTX å¹»ç‡ˆç‰‡æ‘˜è¦é¡å‹
            elif doc_type == "pptx_slide":
                slide_id = doc_content.get("slide_id", "unknown_slide")
                summary = doc_content.get("summary", "ç„¡æ‘˜è¦")
                slide_summaries.append(f"ã€å¹»ç‡ˆç‰‡: {slide_id}ã€‘\n{summary}")

            # è™•ç†æ–‡æœ¬æˆ–è¡¨æ ¼é¡å‹
            elif doc_type in ["text", "table"]:
                content = doc_content.get("content", "")
                if content:
                    texts.append(content)

            # è™•ç†æœªçŸ¥é¡å‹çš„å­—å…¸
            else:
                logger.warning(f"âš ï¸ Unknown dictionary type: {doc_content}")

        # è™•ç†å­—ç¬¦ä¸²é¡å‹ (å‘å¾Œå…¼å®¹èˆŠæ ¼å¼)
        elif isinstance(doc_content, str):
            texts.append(doc_content)
        else:
            logger.warning(f"âš ï¸ Unexpected document type received: {type(doc_content)}")

    # å¢åŠ è™•ç†çµæœç¸½çµ
    if len(b64_images) == 0 and any(
        isinstance(d, dict) and "filename" in d for d in docs
    ):
        logger.warning(
            "âš ï¸ Warning: Image references were found but no images could be loaded!"
        )

    # å°‡é é¢å’Œå¹»ç‡ˆç‰‡æ‘˜è¦ä¹Ÿæ·»åŠ åˆ°æ–‡æœ¬åˆ—è¡¨ä¸­
    texts.extend(page_summaries)
    texts.extend(slide_summaries)  # æ–°å¢ï¼šåŠ å…¥å¹»ç‡ˆç‰‡æ‘˜è¦

    return {"images": b64_images, "texts": texts}


def img_prompt_func(data_dict):
    """
    çµ„åˆå¤šæ¨¡æ…‹ LLM è¼¸å…¥ prompt (é€šç”¨ç‰ˆæœ¬)ã€‚
    data_dict: dictï¼Œå« context èˆ‡ question
    å›å‚³ï¼šLLM è¼¸å…¥è¨Šæ¯åˆ—è¡¨
    å°‡æª¢ç´¢åˆ°çš„åœ–ç‰‡èˆ‡æ–‡å­—çµ„åˆæˆ LLM å¯ç†è§£çš„æ ¼å¼ã€‚
    """
    # å¾ context ä¸­æå–æ–‡å­—/è¡¨æ ¼å…§å®¹
    formatted_texts = "\n".join(data_dict["context"]["texts"])
    # å¾ context ä¸­æå–åœ–ç‰‡ (base64)
    images = data_dict["context"]["images"]
    # ç²å–ä½¿ç”¨è€…çš„å•é¡Œ
    question = data_dict["question"]

    # --- Prompt è¨­è¨ˆ ---
    # 1. è¨­å®šè§’è‰²/ç›®æ¨™ (é€šç”¨)
    role_description = (
        "æ‚¨æ˜¯ä¸€ä½ AI åŠ©æ‰‹ï¼Œæ“…é•·åˆ†æå’Œæ•´åˆæä¾›çš„å¤šæ¨¡æ…‹è³‡è¨Šï¼ˆåŒ…å«æ–‡å­—ã€è¡¨æ ¼å’Œåœ–åƒï¼‰ã€‚"
    )

    # 2. æè¿°è¼¸å…¥
    input_description = "æ‚¨å°‡æ”¶åˆ°ä»¥ä¸‹å…§å®¹ï¼š"
    if images:
        input_description += "\n- ä¸€æˆ–å¤šå¼µåœ–åƒã€‚"
    if formatted_texts.strip():  # æª¢æŸ¥æ˜¯å¦æœ‰éç©ºç™½æ–‡å­—
        input_description += "\n- ç›¸é—œçš„æ–‡å­—æ®µè½å’Œ/æˆ–è¡¨æ ¼ã€‚"

    # 3. è¨­å®šä»»å‹™æŒ‡ä»¤
    task_instruction = f"""è«‹æ ¹æ“šä»¥ä¸Šæä¾›çš„æ‰€æœ‰è³‡è¨Šï¼ˆåœ–åƒã€æ–‡å­—ã€è¡¨æ ¼ï¼‰ï¼Œç²¾ç¢ºä¸”åƒ…åŸºæ–¼é€™äº›è³‡è¨Šä¾†å›ç­”ä»¥ä¸‹å•é¡Œï¼š
"{question}"

è«‹ç¢ºä¿æ‚¨çš„å›ç­”ç›´æ¥å›æ‡‰å•é¡Œï¼Œä¸¦æ•´åˆä¾†è‡ªä¸åŒä¾†æºçš„ç›¸é—œç´°ç¯€ï¼Œä¸”ä¸€å¾‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚å¦‚æœæä¾›çš„è³‡è¨Šä¸è¶³ä»¥å›ç­”å•é¡Œï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚"""
    # --- Prompt çµ„åˆ ---
    final_prompt_text = f"{role_description}\n\n{input_description}\n\n{task_instruction}\n\næä¾›çš„æ–‡å­—èˆ‡è¡¨æ ¼å…§å®¹ï¼š\n------\n{formatted_texts}\n------"

    # --- å»ºç«‹ LangChain è¨Šæ¯ ---
    messages = []

    # 1. æ·»åŠ åœ–åƒè¨Šæ¯ (å¦‚æœæœ‰çš„è©±)
    if images:
        for image in images:
            image_message = {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image}"},
            }
            messages.append(image_message)

    # 2. æ·»åŠ æ–‡å­—è¨Šæ¯ (åŒ…å«å®Œæ•´çš„ prompt)
    text_message = {
        "type": "text",
        "text": final_prompt_text,
    }
    messages.append(text_message)

    # è¿”å› HumanMessage åˆ—è¡¨
    return [HumanMessage(content=messages)]


def multi_modal_rag_chain(retriever, figure_dir=IMAGE_FIGURES_PATH):
    """
    å»ºç«‹å¤šæ¨¡æ…‹ RAG æ¨ç†éˆã€‚
    retriever: å¤šå‘é‡æª¢ç´¢å™¨
    å›å‚³ï¼šchainï¼ˆRAG æ¨ç†éˆï¼‰
    """

    # Multi-modal LLM
    model = gpt_4o()

    # RAG pipeline
    chain = (
        {
            "context": retriever
            | RunnableLambda(lambda docs: split_image_text_types(docs, figure_dir)),
            "question": RunnablePassthrough(),
        }
        | RunnableLambda(img_prompt_func)
        | model
        | StrOutputParser()
    )

    return chain


# --- Main Execution Logic ---

# 1. Load the Vector Store (FAISS)
logger.info("ğŸ”„ Loading FAISS Vector Store...")
try:
    faiss_actual_index_file = os.path.join(
        FAISS_INDEX_PATH, f"{FAISS_INDEX_NAME}.faiss"
    )
    if os.path.exists(faiss_actual_index_file):
        vectorstore = FAISS.load_local(
            FAISS_INDEX_PATH,
            text_embedding_3_large,
            FAISS_INDEX_NAME,
            allow_dangerous_deserialization=True,
        )
        logger.info("âœ… FAISS Vector Store Loaded.")
    else:
        logger.warning("âš ï¸ FAISS index not found. Creating placeholder index...")
        os.makedirs(FAISS_INDEX_PATH, exist_ok=True)
        # Create a dummy FAISS index
        initial_doc_texts = ["placeholder document for faiss initialization"]
        vectorstore = FAISS.from_texts(initial_doc_texts, text_embedding_3_large)
        vectorstore.save_local(FAISS_INDEX_PATH, FAISS_INDEX_NAME)
        logger.info("âœ… Placeholder FAISS Vector Store Created.")
except Exception as e:
    logger.error(f"âŒ Error loading FAISS vector store: {e}")
    logger.info("Creating new placeholder FAISS index...")
    os.makedirs(FAISS_INDEX_PATH, exist_ok=True)
    initial_doc_texts = ["placeholder document for faiss initialization"]
    vectorstore = FAISS.from_texts(initial_doc_texts, text_embedding_3_large)
    vectorstore.save_local(FAISS_INDEX_PATH, FAISS_INDEX_NAME)
    logger.info("âœ… New FAISS Vector Store Created.")

# 2. Load the Docstore Mapping
logger.info(f"ğŸ”„ Loading Docstore Mapping from {DOCSTORE_MAPPING_PATH}...")
try:
    with open(DOCSTORE_MAPPING_PATH, "r", encoding="utf-8") as f:
        loaded_mapping = json.load(f)
    logger.info("âœ… Docstore Mapping Loaded.")
except FileNotFoundError:
    logger.warning(
        f"âš ï¸ Warning: Docstore mapping file not found at {DOCSTORE_MAPPING_PATH}. Creating empty mapping."
    )
    # åˆ›å»ºç©ºçš„æ˜ å°„å’Œå¿…è¦çš„ç›®å½•
    os.makedirs(os.path.dirname(DOCSTORE_MAPPING_PATH), exist_ok=True)
    loaded_mapping = {}
    with open(DOCSTORE_MAPPING_PATH, "w", encoding="utf-8") as f:
        json.dump(loaded_mapping, f, ensure_ascii=False, indent=4)
except json.JSONDecodeError:
    logger.error(
        f"âŒ Error: Could not decode JSON from {DOCSTORE_MAPPING_PATH}. Creating empty mapping."
    )
    loaded_mapping = {}
    with open(DOCSTORE_MAPPING_PATH, "w", encoding="utf-8") as f:
        json.dump(loaded_mapping, f, ensure_ascii=False, indent=4)
except Exception as e:
    logger.error(f"âŒ An unexpected error occurred while loading the mapping: {e}")
    logger.info("Creating empty mapping.")
    loaded_mapping = {}
    with open(DOCSTORE_MAPPING_PATH, "w", encoding="utf-8") as f:
        json.dump(loaded_mapping, f, ensure_ascii=False, indent=4)


# 3. Reconstruct the InMemoryStore and Populate it
logger.info("ğŸ”„ Reconstructing Docstore...")
store = InMemoryStore()
store.mset(list(loaded_mapping.items()))  # Populate the store
logger.info("âœ… Docstore Reconstructed.")

# 4. Initialize the MultiVectorRetriever CORRECTLY
logger.info("ğŸ”„ Initializing MultiVectorRetriever...")
retriever_multi_vector_img = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=store,
    id_key="doc_id",  # Must match the key used when storing documents
)
logger.info("âœ… MultiVectorRetriever Initialized.")


# 5. Build the RAG Chain
logger.info("ğŸ”„ Building RAG Chain...")
chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)
logger.info("âœ… RAG Chain Built.")

# å°‡æ¸¬è©¦æŸ¥è©¢ä»£ç¢¼åŒ…è£åœ¨ if __name__ == "__main__" å…§ï¼Œé¿å…åœ¨è¢«å°å…¥æ™‚åŸ·è¡Œ
if __name__ == "__main__":
    # 6. Run a Query
    query = ""  # Example query
    logger.info(f"\nâ“ Running Query: {query}\n")

    # Optional: Check what the retriever fetches directly

    logger.info("--- Retriever Output ---")
    faiss_index_size = getattr(vectorstore.index, "ntotal", "unknown")
    start_time = time.perf_counter()
    retrieved_docs = retriever_multi_vector_img.invoke(query)
    elapsed = time.perf_counter() - start_time
    logger.info(
        f"Retrieved {len(retrieved_docs)} documents in {elapsed:.2f}s (FAISS index size: {faiss_index_size})"
    )
    for i, item in enumerate(retrieved_docs):
        logger.info(f"--- Item {i+1} ---")

        if isinstance(item, str):
            # å¦‚æœæ˜¯å­—ä¸²ï¼Œæ¨™ç¤ºç‚ºæ–‡å­—å€å¡Šä¸¦å°å‡ºå…§å®¹
            logger.info("[Type: Text Block]")
            logger.info("Content:")
            logger.info(item.strip())

        elif isinstance(item, dict) and "filename" in item:
            logger.info("[Type: Image Reference]")
            filename = item["filename"]
            # Safely get the summary, provide default if missing
            summary = item.get("summary", "[Summary not found in docstore]")
            logger.info(f"Referenced Image File: {filename}")
            logger.info(f"Image Summary: {summary}")

        else:
            # è™•ç†æœªé æœŸçš„é¡å‹
            logger.info(f"[Type: Unknown ({type(item)})]")
            logger.info(f"Content: {item}")

        logger.info("-" * 20)
        logger.info("")

    logger.info("------------------------\n")

    logger.info("--- RAG Chain Output ---")
    for chunk in chain_multimodal_rag.stream(query):
        logger.info(chunk)
    logger.info("\n------------------------")
    logger.info("\nâœ… Query Finished.")
