import os
from dotenv import load_dotenv
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")


import json
import base64

from vector_store import  resize_base64_image
from summarize import encode_image

from langchain.retrievers.multi_vector import MultiVectorRetriever 
from langchain.storage import InMemoryStore
from langchain_core.output_parsers import StrOutputParser
from langchain_openai.chat_models import ChatOpenAI
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.messages import HumanMessage
from langchain_core.documents import Document

VECTORSTORE_PATH = "./chroma_store"
DOCSTORE_MAPPING_PATH = "./docstore_mapping.json"
IMAGE_FIGURES_PATH = "figures/"

def split_image_text_types(docs):
    """
    å°‡ MultiVectorRetriever æª¢ç´¢å›ä¾†çš„åŸå§‹å…§å®¹åˆ†ç‚ºåœ–ç‰‡ (base64) èˆ‡æ–‡å­—/è¡¨æ ¼ã€‚
    docs: æª¢ç´¢å›ä¾†çš„æ–‡ä»¶åˆ—è¡¨ (åŒ…å«åŸå§‹æ–‡å­—ã€è¡¨æ ¼å­—ä¸²ï¼Œæˆ–åœ–ç‰‡çš„ {'filename': '...'} å­—å…¸)
    å›å‚³ï¼šdictï¼Œå« imagesï¼ˆbase64 åœ–ç‰‡åˆ—è¡¨ï¼‰ã€textsï¼ˆæ–‡å­—/è¡¨æ ¼åˆ—è¡¨ï¼‰
    """
    b64_images = []
    texts = []
    for doc in docs:
        if isinstance(doc, Document): # Handle cases where retriever might still wrap in Document
             doc_content = doc.page_content
        else:
             doc_content = doc # Assume it's the raw content from the docstore

        # Check if it's an image reference dictionary stored by our custom logic
        if isinstance(doc_content, dict) and 'filename' in doc_content:
            img_filename = doc_content['filename']
            img_path = os.path.join(IMAGE_FIGURES_PATH, img_filename)
            if os.path.exists(img_path):
                try:
                    # Encode the image from file path
                    b64_img = encode_image(img_path)
                    # Resize the base64 image
                    resized_b64_img = resize_base64_image(b64_img, size=(1300, 600)) # Adjust size as needed
                    b64_images.append(resized_b64_img)
                except Exception as e:
                    print(f"âš ï¸ Error processing image file {img_path}: {e}")
            else:
                print(f"âš ï¸ Image file not found: {img_path}")
        # Otherwise, treat it as text or table string
        elif isinstance(doc_content, str):
            texts.append(doc_content)
        else:
             print(f"âš ï¸ Unexpected document type received: {type(doc_content)}")

    return {"images": b64_images, "texts": texts}


def img_prompt_func(data_dict):
    """
    çµ„åˆå¤šæ¨¡æ…‹ LLM è¼¸å…¥ prompt (é€šç”¨ç‰ˆæœ¬)ã€‚
    data_dict: dictï¼Œå« context èˆ‡ question
    å›å‚³ï¼šLLM è¼¸å…¥è¨Šæ¯åˆ—è¡¨
    å°‡æª¢ç´¢åˆ°çš„åœ–ç‰‡èˆ‡æ–‡å­—çµ„åˆæˆ LLM å¯ç†è§£çš„æ ¼å¼ã€‚
    """
    # å¾ context ä¸­æå–æ–‡å­—/è¡¨æ ¼å…§å®¹
    formatted_texts = "\n".join(data_dict["context"]["texts"])
    # å¾ context ä¸­æå–åœ–ç‰‡ (base64)
    images = data_dict["context"]["images"]
    # ç²å–ä½¿ç”¨è€…çš„å•é¡Œ
    question = data_dict['question']

    # --- Prompt è¨­è¨ˆ ---
    # 1. è¨­å®šè§’è‰²/ç›®æ¨™ (é€šç”¨)
    role_description = "æ‚¨æ˜¯ä¸€ä½ AI åŠ©æ‰‹ï¼Œæ“…é•·åˆ†æå’Œæ•´åˆæä¾›çš„å¤šæ¨¡æ…‹è³‡è¨Šï¼ˆåŒ…å«æ–‡å­—ã€è¡¨æ ¼å’Œåœ–åƒï¼‰ã€‚"

    # 2. æè¿°è¼¸å…¥
    input_description = "æ‚¨å°‡æ”¶åˆ°ä»¥ä¸‹å…§å®¹ï¼š"
    if images:
        input_description += "\n- ä¸€æˆ–å¤šå¼µåœ–åƒã€‚"
    if formatted_texts.strip(): # æª¢æŸ¥æ˜¯å¦æœ‰éç©ºç™½æ–‡å­—
        input_description += "\n- ç›¸é—œçš„æ–‡å­—æ®µè½å’Œ/æˆ–è¡¨æ ¼ã€‚"

    # 3. è¨­å®šä»»å‹™æŒ‡ä»¤
    task_instruction = f"""è«‹æ ¹æ“šä»¥ä¸Šæä¾›çš„æ‰€æœ‰è³‡è¨Šï¼ˆåœ–åƒã€æ–‡å­—ã€è¡¨æ ¼ï¼‰ï¼Œç²¾ç¢ºä¸”åƒ…åŸºæ–¼é€™äº›è³‡è¨Šä¾†å›ç­”ä»¥ä¸‹å•é¡Œï¼š
"{question}"

è«‹ç¢ºä¿æ‚¨çš„å›ç­”ç›´æ¥å›æ‡‰å•é¡Œï¼Œä¸¦æ•´åˆä¾†è‡ªä¸åŒä¾†æºçš„ç›¸é—œç´°ç¯€ã€‚å¦‚æœæä¾›çš„è³‡è¨Šä¸è¶³ä»¥å›ç­”å•é¡Œï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚"""
    # --- Prompt çµ„åˆ ---
    final_prompt_text = f"{role_description}\n\n{input_description}\n\n{task_instruction}\n\næä¾›çš„æ–‡å­—èˆ‡è¡¨æ ¼å…§å®¹ï¼š\n------\n{formatted_texts}\n------"

    # --- å»ºç«‹ LangChain è¨Šæ¯ ---
    messages = []

    # 1. æ·»åŠ åœ–åƒè¨Šæ¯ (å¦‚æœæœ‰çš„è©±)
    if images:
        for image in images:
            image_message = {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image}"},
            }
            messages.append(image_message)

    # 2. æ·»åŠ æ–‡å­—è¨Šæ¯ (åŒ…å«å®Œæ•´çš„ prompt)
    text_message = {
        "type": "text",
        "text": final_prompt_text,
    }
    messages.append(text_message)

    # è¿”å› HumanMessage åˆ—è¡¨
    return [HumanMessage(content=messages)]



def multi_modal_rag_chain(retriever):
    """
    å»ºç«‹å¤šæ¨¡æ…‹ RAG æ¨ç†éˆã€‚
    retriever: å¤šå‘é‡æª¢ç´¢å™¨
    å›å‚³ï¼šchainï¼ˆRAG æ¨ç†éˆï¼‰
    """

    # Multi-modal LLM
    model = ChatOpenAI(temperature=0, model="gpt-4o", max_tokens=1024, api_key=api_key)

    # RAG pipeline
    chain = (
        {
            "context": retriever | RunnableLambda(split_image_text_types),
            "question": RunnablePassthrough(),
        }
        | RunnableLambda(img_prompt_func)
        | model
        | StrOutputParser()
    )

    return chain



# --- Main Execution Logic ---

# 1. Load the Vector Store
print("ğŸ”„ Loading Vector Store...")
vectorstore = Chroma(
    collection_name="mm_rag",
    embedding_function=OpenAIEmbeddings(),
    persist_directory=VECTORSTORE_PATH
)
print("âœ… Vector Store Loaded.")

# 2. Load the Docstore Mapping
print(f"ğŸ”„ Loading Docstore Mapping from {DOCSTORE_MAPPING_PATH}...")
try:
    with open(DOCSTORE_MAPPING_PATH, 'r', encoding='utf-8') as f:
        loaded_mapping = json.load(f)
    print("âœ… Docstore Mapping Loaded.")
except FileNotFoundError:
    print(f"âŒ Error: Docstore mapping file not found at {DOCSTORE_MAPPING_PATH}. Did you run build_vector_db.py first?")
    exit()
except json.JSONDecodeError:
    print(f"âŒ Error: Could not decode JSON from {DOCSTORE_MAPPING_PATH}.")
    exit()
except Exception as e:
    print(f"âŒ An unexpected error occurred while loading the mapping: {e}")
    exit()


# 3. Reconstruct the InMemoryStore and Populate it
print("ğŸ”„ Reconstructing Docstore...")
store = InMemoryStore()
store.mset(list(loaded_mapping.items())) # Populate the store
print("âœ… Docstore Reconstructed.")

# 4. Initialize the MultiVectorRetriever CORRECTLY
print("ğŸ”„ Initializing MultiVectorRetriever...")
retriever_multi_vector_img = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=store,
    id_key="doc_id", # Make sure this matches the key used in build_vector_db.py/vector_store.py
)
print("âœ… MultiVectorRetriever Initialized.")


# 5. Build the RAG Chain
print("ğŸ”„ Building RAG Chain...")
chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)
print("âœ… RAG Chain Built.")

# 6. Run a Query
query = "æ„›æƒ…äº¤å‹çš„å—å®³è€…äººæ•¸" # Example query
print(f"\nâ“ Running Query: {query}\n")

# Optional: Check what the retriever fetches directly
print("--- Retriever Output ---")
retrieved_docs = retriever_multi_vector_img.invoke(query)
for i, item in enumerate(retrieved_docs):
    print(f"--- Item {i+1} ---") # ç‚ºæ¯å€‹é …ç›®åŠ ä¸Šç·¨è™Ÿå’Œåˆ†éš”ç·š

    if isinstance(item, str):
        # å¦‚æœæ˜¯å­—ä¸²ï¼Œæ¨™ç¤ºç‚ºæ–‡å­—å€å¡Šä¸¦å°å‡ºå…§å®¹
        print("[Type: Text Block]")
        print("Content:")
        print(item.strip()) # ä½¿ç”¨ strip() å»é™¤å‰å¾Œå¤šé¤˜ç©ºç™½

    elif isinstance(item, dict) and 'filename' in item:
        print("[Type: Image Reference]")
        filename = item['filename']
        # Safely get the summary, provide default if missing
        summary = item.get('summary', '[Summary not found in docstore]')
        print(f"Referenced Image File: {filename}")
        print(f"Image Summary: {summary}") # Print the summary
    

    else:
        # è™•ç†æœªé æœŸçš„é¡å‹
        print(f"[Type: Unknown ({type(item)})]")
        print(f"Content: {item}")

    print("-" * 20) # æ¯å€‹é …ç›®ä¹‹é–“çš„åˆ†éš”ç·š
    print() # å¢åŠ ç©ºè¡Œï¼Œè®“è¼¸å‡ºæ›´æ¸…æ™°

print("------------------------\n")


print("--- RAG Chain Output ---")
for chunk in chain_multimodal_rag.stream(query):
    print(chunk, flush=True, end="")
print("\n------------------------")
print("\nâœ… Query Finished.")
